{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from spacy.cli import download\n",
    "import unicodedata\n",
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import matutils, models, corpora, similarities\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f705712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4337172",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('en_core_web_sm')\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "tokenizer=ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile=open('preprocessed_stack','rb')\n",
    "stack_df=pickle.load(picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile=open('stack_dataframe_cleaned-1','rb')\n",
    "stack_df=pickle.load(picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df.drop_duplicates(subset='title',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(txt):\n",
    "    list1=txt.split('|')\n",
    "    return list1\n",
    "    #return txt.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df['tags'][0].split('|') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df['list_tags']=stack_df['tags'].apply(lambda x: split(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df[stack_df['views']>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.histogram(stack_df[stack_df['reputation']>5000],x='reputation',title='Reputation Counts')\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename='reputation_hist.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.histogram(stack_df,x='qa',title='Text Counts')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile=open('stack_dataframe_cleaned-1','wb')\n",
    "pickle.dump(stack_df,picklefile)\n",
    "picklefile.close()\n",
    "#stack_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech=['POS','ADP','ADV','AUX','CONJ','CCONJ','DET','INTJ','NUM','PART','PUNCT','SCONJ','SYM','X','SPACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfa437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html tag and escaping code\n",
    "def clean_html(txt):\n",
    "    soup=BeautifulSoup(txt,\"lxml\")\n",
    "    [x.extract() for x in soup.find_all('code')]\n",
    "    [x.extract() for x in soup.find_all('script')]\n",
    "    [x.extract() for x in soup.find_all('style')]\n",
    "    [x.extract() for x in soup.find_all('meta')]\n",
    "    [x.extract() for x in soup.find_all('noscript')]\n",
    "    [x.extract() for x in soup.find_all(text=lambda text:isinstance(text, Comment))]\n",
    "    res=soup.get_text()\n",
    "    return soup.get_text()\n",
    "def remove1(txt,nlp):\n",
    "    list_text=[]\n",
    "    txt1=nlp(txt)\n",
    "    for word in txt1:\n",
    "        if (word.pos_ not in parts_of_speech):\n",
    "            if word.text != 'I':\n",
    "                list_text.append(word.text)\n",
    "    return list_text\n",
    "def remove2(txt,nlp):\n",
    "    list_text=[]\n",
    "    txt1=nlp(txt)\n",
    "    for word in txt1:\n",
    "        if (word.pos_ not in parts_of_speech):\n",
    "            if word.text != 'I':\n",
    "                list_text.append(word.text)\n",
    "    join_text=\" \".join(list_text)\n",
    "    return join_text\n",
    "\n",
    "def remove_accented_chars(txt):\n",
    "    txt=unicodedata.normalize('NFKD', txt).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return txt\n",
    "\n",
    "def full_clean(txt,nlp):\n",
    "    txt=remove2(txt,nlp)\n",
    "    txt=txt.lower()\n",
    "    #remove unicode characters\n",
    "    txt=txt.encode('ascii','ignore').decode()\n",
    "    #remove accent\n",
    "    txt=remove_accented_chars(txt)\n",
    "    #remove links\n",
    "    txt=re.sub(r'http*\\S+','',txt)\n",
    "    txt=regexp_tokenize(txt,pattern=r\"\\s|[\\.,;']\", gaps=True)\n",
    "    \n",
    "    stop_words=stopwords.words(\"english\")\n",
    "    txt=[elem for elem in txt if elem not in stop_words]\n",
    "    lemma=nltk.WordNetLemmatizer()\n",
    "    #txt=lemma.lemmatize(txt)\n",
    "    txt=[lemma.lemmatize(word) for word in txt]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f18c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt1=nlp(stack_df['qa'][0])\n",
    "stack_df['text']=stack_df['qa'].apply(lambda x:full_clean(x,nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb099a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_topics=20'#min_df originally 10\n",
    "tf_vect=CountVectorizer(max_df=0.8, min_df=5, stop_words='english',token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "#max_features=1000, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=tf_vect.fit_transform(stack_df['qa']).toarray()\n",
    "#tf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=stack_df['text']\n",
    "y=stack_df['list_tags'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=TfidfVectorizer(analyzer='word', max_df=.9,min_df=0.01,tokenizer=None,\n",
    "                    preprocessor=' '.join,stop_words=None,lowercase=False)\n",
    "vect.fit(X)\n",
    "X_tfidf=vect.fit_transform(X).toarray()\n",
    "#print(X_tfidf)\n",
    "#len(vect.get_feature_names())\n",
    "#print(f'shape of X for text: {X_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fd784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_test,y_train,y_test=train_test_split(X_tfidf,y_bin,test_size=0.3,random_state=42)\n",
    "#print(f'X_train shape: {X_train.shape}')\n",
    "#print(f'X_test shape: {X_test.shape}')\n",
    "#print(f'y_train shape: {y_train.shape}')\n",
    "#print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=10\n",
    "tfidf=X_tfidf\n",
    "lda=LatentDirichletAllocation(n_components=n_topics, max_iter=5,learning_method='online',\n",
    "                             learning_offset=50.,random_state=42)\n",
    "lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top n words for each topic identified\n",
    "def display_topics(model, features, words_count):\n",
    "    for topic_no, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_no))\n",
    "        print(\" \".join([features[i] for i in topic.argsort()[:-words_count - 1:-1]]))\n",
    "def display_topics2(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(topic.argsort()[:-no_top_words - 1:-1])\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "   # return pd.DataFrame(topic_dict)\n",
    "              \n",
    "words_count=10\n",
    "# Display top 10 words for each topic\n",
    "#display_topics(lda, vect.get_feature_names(), words_count)\n",
    "no_top_words = 20\n",
    "#display_topics2(lda, vect.get_feature_names_out(), no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
