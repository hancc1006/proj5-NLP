{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import DBSCAN, SpectralClustering, MeanShift\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47ad56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ee671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from office hours nlp_pipline-ipynb\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "#try to play around with less topics top 5 words, top 6 words\n",
    "no_top_words = 30\n",
    "topic_df=display_topics(lda, vect.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=display_topics(lda, tf_vect.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f607cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics1(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict\n",
    "topics=display_topics1(lda, tf_vect.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors=list('rgbcmyk')\n",
    "#x1=topics.keys()\n",
    "#y1=topics.values()\n",
    "#plt.scatter(x1,y1,colors=colors.pop())\n",
    "#plt.legend(topics.keys())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#term document matrix for\n",
    "topic_df.head(3)\n",
    "#organize tableau-formatted table\n",
    "#one slide per 1 chart\n",
    "#automate circle \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "#plot on scatter plot\n",
    "#dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/ismailazdad/P5_NLP_stackoverflow/blob/main/02_unsupervized_nlp.ipynb\n",
    "#to get the visualization \n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    #affichage des 500 premiers motes par topics\n",
    "    plt.subplots(figsize=(12,8))\n",
    "    print(\"Topic {}:\".format(topic_idx))\n",
    "    print(\" \".join([vect.get_feature_names_out()[i] for i in topic.argsort()[:-30 - 1:-1]]))\n",
    "    text = \" \".join([vect.get_feature_names_out()[i] for i in topic.argsort()[:-500 - 1:-1]])\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    #plt.savefig(f'topic_wordcloud_{topic_idx}.jpg',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    #affichage des 500 premiers motes par topics\n",
    "    plt.subplots(figsize=(12,8))\n",
    "    print(\"Topic {}:\".format(topic_idx))\n",
    "    print(\" \".join([tf_vect.get_feature_names_out()[i] for i in topic.argsort()[:-30 - 1:-1]]))\n",
    "    text = \" \".join([tf_vect.get_feature_names_out()[i] for i in topic.argsort()[:-500 - 1:-1]])\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    #plt.savefig(f'topic_wordcloud_{topic_idx}.jpg',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#term document matrix for tfidf\n",
    "tfidf_df=pd.DataFrame(X_tfidf,columns=vect.get_feature_names()).transpose()\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41349a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#term document matrix for tfidf\n",
    "tfidf_df=pd.DataFrame(tf,columns=vect.get_feature_names()).transpose()\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c863558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def get_high_dim_data(k,num_dim=20,num_points=100):\n",
    "    np.random.seed(9)\n",
    "    data = []\n",
    "    modifiers = []\n",
    "    for i in range(0,k):\n",
    "        modifiers = np.random.randint(k,size=num_dim)\n",
    "        print(modifiers)\n",
    "        for _ in range(0,num_points):\n",
    "            data_vals = []\n",
    "            for j in range(num_dim):\n",
    "                data_vals.append(np.random.normal(modifiers[j]*i))   \n",
    "            data.append(data_vals)\n",
    "    return data\n",
    "\n",
    "\n",
    "data = get_high_dim_data(10,num_dim=3)\n",
    "x,y,z = zip(*data)\n",
    "plt.style.use(\"seaborn-poster\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x,y,z)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "num_points=len(tf.toarray())\n",
    "num_dim=10\n",
    "data = []\n",
    "modifiers = []\n",
    "for i in range(0,k):\n",
    "    #modifiers = np.random.randint(k,size=num_dim)\n",
    "    for _ in range(0,num_points):\n",
    "        data_vals = []\n",
    "        for j in range(num_dim):\n",
    "            data_vals.append(np.random.normal(tf.toarray()[j]*i))   \n",
    "        data.append(data_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "num_clust = 10\n",
    "num_dim = 30\n",
    "#high_data = get_high_dim_data(num_clust, num_points=num_points, num_dim=num_dim)\n",
    "model = TSNE(n_components=2, random_state=0,verbose=2)\n",
    "low_data = model.fit_transform([data1 for data1 in data])\n",
    "\n",
    "num_points=int(len(tfidf_df)/10)+2\n",
    "colorize = []\n",
    "for i in range(num_clust):\n",
    "    for _ in range(num_points):\n",
    "        colorize.append(plt.cm.rainbow(i*20))\n",
    "x,y = zip(*low_data)\n",
    "#plt.scatter(x,y,c=colorize,s=40)\n",
    "plt.scatter(x,y,s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b26658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model = TSNE(n_components=3, learning_rate=100, random_state=0,verbose=2)\n",
    "tsne_features=model.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f5b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_features\n",
    "tsne_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile=open('tsne_3dim_X_tfidf','wb')\n",
    "pickle.dump(tsne_features,picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf46326",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points=int(len(tsne_features)/6)\n",
    "num_clust=6\n",
    "colorize = []\n",
    "for i in range(num_clust):\n",
    "    for _ in range(num_points):\n",
    "        colorize.append(plt.cm.rainbow(i*20))\n",
    "x,y,z = zip(*tsne_features)\n",
    "plt.scatter(x,y,c=colorize,s=40)\n",
    "#plt.scatter(x,y,s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model = TSNE(n_components=3, learning_rate=100, random_state=0,verbose=2)\n",
    "tsne_features2=model.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile=open('tsne_3dim_count','wb')\n",
    "pickle.dump(tsne_features2,picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ef37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = zip(*tsne_features2)\n",
    "plt.style.use(\"seaborn-poster\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x,y,z)\n",
    "plt.savefig(f'3D plot of tsne features.jpg',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44821314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model = TSNE(n_components=2, learning_rate=100, random_state=0,verbose=2)\n",
    "tsne_features1=model.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd38423",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile=open('tsne_2dim_count','wb')\n",
    "pickle.dump(tsne_features1,picklefile)\n",
    "picklefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = zip(*tsne_features1)\n",
    "plt.style.use(\"seaborn-poster\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b38409",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points=int(len(tfidf_df)/10)\n",
    "print(num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7041efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(k,num_points=len(tsne_features)):\n",
    "    np.random.seed(9)\n",
    "    data = []\n",
    "    for i in range(0,k):\n",
    "        for _ in range(0,num_points):\n",
    "            data.append([tsne_features[:,0],tsne_features[:,1]])\n",
    "    x1,y1 = zip(*data)\n",
    "    #plt.xlabel('X')\n",
    "    #plt.ylabel('Y')\n",
    "    #plt.scatter(x1,y1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(6)\n",
    "x,y = zip(*data)\n",
    "plt.scatter(x, y, s=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf__tf=tfidf_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize our data for DBSCAN and fit DBSCAN\n",
    "\n",
    "## Remember to scale!  Why??   # Standardize features by removing the mean and scaling to unit variance.\n",
    "#https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering\n",
    "X = StandardScaler().fit_transform(tf)\n",
    "\n",
    "db = DBSCAN(eps=5, min_samples=5).fit(X)\n",
    "## Note, how can we choose eps? \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html\n",
    "\n",
    "# Let's find the observations DBSCAN called \"core\"\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "labels = db.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "plt.figure(dpi=200)\n",
    "show_core = True\n",
    "show_non_core = True\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "    if show_core:\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        x, y = xy[:,0], xy[:,1]\n",
    "        plt.scatter(x, y, color=col, edgecolors='k',  s=20, linewidths=1.1) # add black border for core points\n",
    "    \n",
    "    if show_non_core:\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        x, y = xy[:,0], xy[:,1]\n",
    "        plt.scatter(x, y, color=col, s=20, linewidths=1.1)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=.19, min_samples=5).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05325f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clusters assigned are:\", set(db.labels_))\n",
    "\n",
    "uni, counts = np.unique(arr, return_counts=True)\n",
    "d = dict(zip(uni, counts))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=3)\n",
    "pca.fit(tfidf__tf)\n",
    "pca.components_\n",
    "pca.explained_variance_ratio_\n",
    "plt.scatter(pca.components_[0,:],pca.components_[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa=TruncatedSVD(2)\n",
    "lsa.fit(tfidf__tf)\n",
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_test=lsa.transform(tfidf__tf)\n",
    "plt.scatter(plot_test[:,0],plot_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "new_coords=pca.fit_transform(tfidf__tf)\n",
    "dist=pairwise_distances(new_coords[0].reshape(1,-1),new_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc331a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "argsort_tf=dist.argsort()\n",
    "argsort_tf[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx=tfidf__tf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=[]\n",
    "for i in range(1,11):\n",
    "    topics.append(tfx.iloc[argsort_tf[0][i]]['index'])\n",
    "    \n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(tfidf_df)\n",
    "num_clusters=6\n",
    "km=KMeans(n_clusters=num_clusters, random_state=10, n_init=5)\n",
    "km_vect=km.fit(X)\n",
    "km_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df['text'].value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code\n",
    "# 10 popular movies (from 80 unique movies, 19449 users, 25441 rows)\n",
    "#df['name'].value_counts()[0:10].plot('barh', figsize=[10,6], fontsize=20).invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284581f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corextopic import corextopic as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105655bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model=ct.Corex(n_hidden=6,words=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71915b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_topics=6\n",
    "anchor_strength=3\n",
    "tfidf=X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57001396",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model=ct.Corex(n_hidden=no_of_topics,words=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe311c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.fit(tfidf,anchors=list(stack_df['text']),anchor_strength=anchor_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ce8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top n words for each topic identified\n",
    "def display_topics(model, words_count):\n",
    "     for i, topic_words in enumerate(model.get_topics(n_words = words_count)):\n",
    "         topic_words = [words[0] for words in topic_words if words[1] > 0]\n",
    "         print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_words)))\n",
    "words_count = 10\n",
    "# Display top 10 words for each topic \n",
    "display_topics(topic_model,words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf17419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "import gensim\n",
    "full_lda_model = gensim.models.ldamulticore.LdaMulticore(\n",
    "    corpus=stack_df['text'],\n",
    "    id2word=id2word,\n",
    "    num_topics=20,\n",
    "    random_state=8,\n",
    "    per_word_topics=True,\n",
    "    workers=4)\n",
    "# Print Perplexity score\n",
    "print('\\nPerplexity: ', full_lda_model.log_perplexity(corpus))\n",
    "\n",
    "#Print Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=full_lda_model, \n",
    "                                     texts=texts, \n",
    "                                     dictionary=id2word, \n",
    "                                     coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=20, init='random', random_state=42)\n",
    "W = nmf.fit_transform(tfidf_df)\n",
    "H = nmf.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b585345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "no_of_topics = 6\n",
    "tfidf = X_tfidf\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_of_topics).fit(tfidf)\n",
    "# Display top n words for each topic identified\n",
    "def display_topics(model, features, words_count):\n",
    "    for topic_no, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_no))\n",
    "        print(\" \".join([features[i] for i in topic.argsort()[:-words_count - 1:-1]]))\n",
    "words_count = 10\n",
    "# Display top 10 words for each topic \n",
    "display_topics(nmf, vect.get_feature_names(), words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29626286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    #affichage des 500 premiers motes par topics\n",
    "    plt.subplots(figsize=(12,8))\n",
    "    print(\"Topic {}:\".format(topic_idx))\n",
    "    print(\" \".join([tf_vect.get_feature_names_out()[i] for i in topic.argsort()[:-30 - 1:-1]]))\n",
    "    text = \" \".join([tf_vect.get_feature_names_out()[i] for i in topic.argsort()[:-500 - 1:-1]])\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    #plt.savefig(f'topic_wordcloud_{topic_idx}.jpg',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    #affichage des 500 premiers motes par topics\n",
    "    plt.subplots(figsize=(12,8))\n",
    "    print(\"Topic {}:\".format(topic_idx))\n",
    "    print(\" \".join([vect.get_feature_names_out()[i] for i in topic.argsort()[:-30 - 1:-1]]))\n",
    "    text = \" \".join([vect.get_feature_names_out()[i] for i in topic.argsort()[:-500 - 1:-1]])\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    #plt.savefig(f'topic_wordcloud_{topic_idx}.jpg',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from office hour recommender systems jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d570c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "items=tfidf_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sims=pd.DataFrame(index=items.columns,columns=items.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac90cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(items.columns)):\n",
    "    # looping through each column for each index\n",
    "    for j in range(len(items.columns)):\n",
    "        items_sims.iloc[i,j]=1-cosine(items.iloc[:,i],items.iloc[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fe3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through our columns:\n",
    "# where cosine ~ cosine distance between item i and item j: \n",
    "\n",
    "def find_sim_items(df1,df2):\n",
    "    for i in range(len(df1.columns)):\n",
    "        for j in range(len(df1.columns)):\n",
    "            # getting cos similiarity by comparing on a row by row basis.. \n",
    "            df2.iloc[i,j]=1-cosine(items.iloc[:,i],items.iloc[:,j])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0458f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sims= find_sim_items(items,items_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Top 5\n",
    "items_sims.columns[np.argsort(items_sims['Code'].values)][-10:-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
